---
suppress-bibliography: true
---

```{console setup_history, include=FALSE}
 export CHAPTER="10"
 export HISTFILE=/history/history_${CHAPTER}
 rm -f $HISTFILE
```


# Polyglot Data Science {#chapter-10-polyglot-data-science}

<!-- What's a polyglot? -->
A polyglot is someone who speaks multiple languages.
A polyglot data scientist, as I see it, is someone who uses multiple programming languages, tools, and techniques to obtain, scrub, explore, and model data.

<!-- Already a polyglot -->
The command line stimulates a polyglot approach.
The command line doesn't care in which programming language a tool is written, as long as they adhere to the Unix philosophy.
We saw that very clearly in [Chapter 4](#chapter-4-creating-reusable-command-line-tools), where we created command-line tools in Bash, Python, and R.
Moreover, we executed SQL queries directly on CSV files and executed R expressions from the command line.
In short, we have already been doing polyglot data science without fully realizing it!

<!-- In this chapter -->
In this chapter I'm going take this further by flipping it around.
I'm going to show you how to leverage the command line from various programming languages and environments.
Because let's be honest, we're not going to spend our entire data science careers at the command line.
As for me, when I'm analysing some data I often use the RStudio IDE and when I'm implementing something, I often use Python.
I use whatever helps me to get the job done.

<!-- Why be polyglot? -->
I find it comforting to know that the command-line is often within arm's reach, without having to switch to a different application.
It allows me to quickly run a command without switching to a separate application and break my workflow.
Examples are downloading files with `curl`, inspecting a piece of data with `head`, creating a backup with `git`, and compiling a website with `make`.
Generally speaking, tasks that normally require a lot of code or simply cannot be done at all without the commandl line.

<!-- A word of caution -->
<!-- Language wars are boring and unproductive. -->
<!-- Integrate into the rest of your workflow or toolset. -->
<!-- Goal is to be productive. -->
<!-- In a digital product or service, it's good idea to rely on as a little as possible, including programming languages. -->
<!-- I believe you should take a pragmatic approach. -->
<!-- The command line is everywhere. -->
<!-- It's a super glue. -->
<!-- Useful for when you're in another environment. -->


## Overview

In this chapter, youâ€™ll learn how to:

* Run a terminal within JupyterLab and RStudio IDE
* Interact with arbitrary command-line tools in Python and R
* Transform RDDs using shell commands in Apache Spark

```{console}
cd /data/ch10
l
```

The instructions to get these files are in [Chapter 2](#chapter-2-getting-started).


<!-- TODO: SHOULD: Write about four modes -->
<!-- ## Four Modes of Leveraging the Command Line in a Language -->

<!-- * run (don't involved data) -->
<!-- * read (receive data from command line) -->
<!-- * write (send data to command line) -->
<!-- * filter (send data to and receive data from command) -->

<!-- Shell capabilities or not? -->




## Leveraging the Command Line in Python

Python is one of the most popular programming languages

<!-- TODO: SHOULD: Add links to all projects, including Jupyter. -->

### Jupyter

Project Jupyter is an open-source project, born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages.
Jupyter supports over 40 programming languages, including Python, R, Julia, and Scala.
In this section I'll focus on Python.

The project includes JupyterLab, Jupyter Notebook, and Jupyter Console.
I'll start with Jupyter Console, as it is the most basic one to work with Python in an interactive way.

<!-- TODO MUST: Annotate console session -->

```{console remove=list(1, "1R"), callouts=c("date", "open", "import", "curl", "rm -v", "upper")}
cat /data/.cache/jupyter-console#!expect_prompt=FALSE
C-C#!literal=FALSE
```

Note:
  returns as list of strings, so to use the value total_lines, get first value and cast to an integer


Jupyter Notebook is, in essence, a browser-based version of Jupyter Console.
It supports the same ways to leverage the command line, including the exclamation mark and bash magic.
The biggest difference is that a notebook cannot only contain code, but also marked-up text, equations, and data visualizations.
It's very popular among data scientists for this reason.
Jupyter Notebook is a separate project and environment, but I'd like to use JupyterLab to work with notebooks, because it offers a more complete IDE.

Figure X is a screenshot of JupyterLab, showing the file explorer (left), a code editor (middle), a notebook (right), and a terminal (bottom). The latter three all show ways to leverage the command line.

```{r jupyterlab, echo=FALSE, fig.cap="JupyterLab with the file explorer, a code editor, a notebook, and a terminal", fig.align="center"}
knitr::include_graphics("images/screenshot_jupyterlab.png")
```

* code
* notebook
* terminal


```{block2, type="rmdwarning"}
This notebook in the screenshot also contains a cell using so-called `%%bash` magic, which allows you to write multi-line Bash scripts.
Because it's much more difficult to use Python variables, I don't recommend this approach.
You're better off creating a Bash script in a separate file, and then executing it by using the exclamation mark (`!`).
```




<!-- TODO: Write out voice recordings -->

### Python Scripts

The subprocess module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This module intends to replace several older modules and functions: os.system

- not run in a shell by default, but is possible to change with `shell` argument to `run` function.

```{console}
bat count.py
```

```{console}
./count.py alice.txt alice
```




## Leveraging the Command Line in R

### RStudio IDE

The RStudio IDE is arguably the most popular environment for working with R.


```{r rstudio-console, echo=FALSE, fig.cap="RStudio IDE with console tab open", fig.align="center"}
knitr::include_graphics("images/screenshot_rstudio_console.png")
```

```{r rstudio-terminal, echo=FALSE, fig.cap="RStudio IDE with terminal tab open", fig.align="center"}
knitr::include_graphics("images/screenshot_rstudio_terminal.png")
```




### R Scripts

```{console}
R --quiet
lines <- readLines("alice.txt")
head(lines)
words <- unlist(strsplit(lines, " "))
head(words)
alice <- system2("grep", c("-i", "alice"), input = words, stdout = TRUE)
head(alice)
length(alice)
```


```{console}
in_con <- pipe("grep b > out")
out_con <- fifo("out")
writeLines(c("foo", "bar"), in_con)
readLines(out_con)
```


```{console}
writeLines(c("baz", "qux"), in_con)
readLines(out_con)
close(out_con);
close(in_con)
q("no")
```

Mention pipe and fread

check out the processx package https://processx.r-lib.org/. experimental at the time of writing. but seems very promising to working with connections in a more robust manner.


<!-- # all four options are executed in a shell.  -->
<!-- # no stdin and no stdout: system -->
<!-- # stdin but no stdout: writeLines(pipe) -->
<!-- # stdout but no stdin: readLines(pipe) -->
<!-- readLines(pipe("")) -->
<!-- # both stdin and stdout: fifo, readLines, writeLines -->
<!-- first-in first-out special file, named pipe -->
<!-- When processes are -->
<!-- exchanging data via the FIFO, the kernel passes all data -->
<!-- internally without writing it to the filesystem. -->


## Leveraging the Command Line Elsewhere

### Clipboard

yank
pbcopy pbpaste

when formatting emailadresses


### Apache Spark

<!-- #TODO: Explain Spark in one or two sentences. Actions and Transformation -->

<!-- TODO: SHOULD include reference to spark and https://scala-lang.org/ -->

Spark is a ...
Through a series of transformations, followed by an action, you build up a pipeline for data processing and machine learning.
It's the system you want to turn to if you want to have you terabytes or petabytes of data.

pipe transformation

I think it's interesting to see the author say that the pipe transformation is one of the more interesting ones.



https://spark.apache.org/docs/latest/rdd-programming-guide.html
pipe(command, [envVars])
 	Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.

- result needs to be converted to integer explicitly
- piping multiple commands is difficult (it's not a shell)
- one item per line

```{console, remove=list("alias", "cache"), callouts=c("textFile", "flatMap", "grep", "wc", "res3", "toInt", "res5")}
alias spark-shell=echo
spark-shell --master local[6]#!enter=FALSE
C-C#!literal=FALSE
cat /data/.cache/spark
```
<1> Read *alice.txt* such that each line is an element.
<2> Split each element on spaces. In other words, each line is split into words.
<3> Pipe each partition through `grep` to keep only the elements that match the string *alice*.
<4> Pipe each partition through `wc` to count the number of elements.
<5> There's one count for each partition.
<6> Sum all counts to get a final count.
<7> The above steps combined into a single command.


Also available in PySpark and

https://stackoverflow.com/questions/54239583/question-about-rdd-pipe-operator-on-apache-spark
It seems, after all, that the external script should be present on all executor nodes. One way to do this is to pass your script via spark-submit (e.g. --files script.sh) and then you should be able to refer that (e.g. "./script.sh") in rdd.pipe.


### Julia

<!-- #TODO: Figure out whether I think this section is necessary -->

Blog post with an introduction: https://blog.leahhanson.us/post/julia/julia-commands.html



### Other IDEs / Notable mensions

- Visual Studio Code https://code.visualstudio.com/docs/editor/integrated-terminal
- Emacs
- VIM (using ! command)
- OS: Guake,
- OS: iTerm2:    https://www.sharmaprakash.com.np/guake-like-dropdown-terminal-in-mac/


## Other Combinations

- reticulate
- Rpy2
- sparkr
- sparklyr
